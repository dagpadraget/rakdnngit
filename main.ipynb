{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dagpadraget/rakdnngit/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MPndApl9IHi",
        "colab_type": "code",
        "outputId": "4eee8b2b-643c-4aa4-cded-ca7183ebc4aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFiVx0jf9KLG",
        "colab_type": "code",
        "outputId": "c9fa6bae-05a5-4ecd-93d3-68d3b72fe618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd content"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wte7rgxF9Unh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -fr rakdnngit\n",
        "!rm -fr tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vLlOpSo7Ta_",
        "colab_type": "code",
        "outputId": "2ea12e42-74ca-4eb1-f064-6b7f324358dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/dagpadraget/rakdnngit"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rakdnngit'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/98)\u001b[K\rremote: Counting objects:   2% (2/98)\u001b[K\rremote: Counting objects:   3% (3/98)\u001b[K\rremote: Counting objects:   4% (4/98)\u001b[K\rremote: Counting objects:   5% (5/98)\u001b[K\rremote: Counting objects:   6% (6/98)\u001b[K\rremote: Counting objects:   7% (7/98)\u001b[K\rremote: Counting objects:   8% (8/98)\u001b[K\rremote: Counting objects:   9% (9/98)\u001b[K\rremote: Counting objects:  10% (10/98)\u001b[K\rremote: Counting objects:  11% (11/98)\u001b[K\rremote: Counting objects:  12% (12/98)\u001b[K\rremote: Counting objects:  13% (13/98)\u001b[K\rremote: Counting objects:  14% (14/98)\u001b[K\rremote: Counting objects:  15% (15/98)\u001b[K\rremote: Counting objects:  16% (16/98)\u001b[K\rremote: Counting objects:  17% (17/98)\u001b[K\rremote: Counting objects:  18% (18/98)\u001b[K\rremote: Counting objects:  19% (19/98)\u001b[K\rremote: Counting objects:  20% (20/98)\u001b[K\rremote: Counting objects:  21% (21/98)\u001b[K\rremote: Counting objects:  22% (22/98)\u001b[K\rremote: Counting objects:  23% (23/98)\u001b[K\rremote: Counting objects:  24% (24/98)\u001b[K\rremote: Counting objects:  25% (25/98)\u001b[K\rremote: Counting objects:  26% (26/98)\u001b[K\rremote: Counting objects:  27% (27/98)\u001b[K\rremote: Counting objects:  28% (28/98)\u001b[K\rremote: Counting objects:  29% (29/98)\u001b[K\rremote: Counting objects:  30% (30/98)\u001b[K\rremote: Counting objects:  31% (31/98)\u001b[K\rremote: Counting objects:  32% (32/98)\u001b[K\rremote: Counting objects:  33% (33/98)\u001b[K\rremote: Counting objects:  34% (34/98)\u001b[K\rremote: Counting objects:  35% (35/98)\u001b[K\rremote: Counting objects:  36% (36/98)\u001b[K\rremote: Counting objects:  37% (37/98)\u001b[K\rremote: Counting objects:  38% (38/98)\u001b[K\rremote: Counting objects:  39% (39/98)\u001b[K\rremote: Counting objects:  40% (40/98)\u001b[K\rremote: Counting objects:  41% (41/98)\u001b[K\rremote: Counting objects:  42% (42/98)\u001b[K\rremote: Counting objects:  43% (43/98)\u001b[K\rremote: Counting objects:  44% (44/98)\u001b[K\rremote: Counting objects:  45% (45/98)\u001b[K\rremote: Counting objects:  46% (46/98)\u001b[K\rremote: Counting objects:  47% (47/98)\u001b[K\rremote: Counting objects:  48% (48/98)\u001b[K\rremote: Counting objects:  50% (49/98)\u001b[K\rremote: Counting objects:  51% (50/98)\u001b[K\rremote: Counting objects:  52% (51/98)\u001b[K\rremote: Counting objects:  53% (52/98)\u001b[K\rremote: Counting objects:  54% (53/98)\u001b[K\rremote: Counting objects:  55% (54/98)\u001b[K\rremote: Counting objects:  56% (55/98)\u001b[K\rremote: Counting objects:  57% (56/98)\u001b[K\rremote: Counting objects:  58% (57/98)\u001b[K\rremote: Counting objects:  59% (58/98)\u001b[K\rremote: Counting objects:  60% (59/98)\u001b[K\rremote: Counting objects:  61% (60/98)\u001b[K\rremote: Counting objects:  62% (61/98)\u001b[K\rremote: Counting objects:  63% (62/98)\u001b[K\rremote: Counting objects:  64% (63/98)\u001b[K\rremote: Counting objects:  65% (64/98)\u001b[K\rremote: Counting objects:  66% (65/98)\u001b[K\rremote: Counting objects:  67% (66/98)\u001b[K\rremote: Counting objects:  68% (67/98)\u001b[K\rremote: Counting objects:  69% (68/98)\u001b[K\rremote: Counting objects:  70% (69/98)\u001b[K\rremote: Counting objects:  71% (70/98)\u001b[K\rremote: Counting objects:  72% (71/98)\u001b[K\rremote: Counting objects:  73% (72/98)\u001b[K\rremote: Counting objects:  74% (73/98)\u001b[K\rremote: Counting objects:  75% (74/98)\u001b[K\rremote: Counting objects:  76% (75/98)\u001b[K\rremote: Counting objects:  77% (76/98)\u001b[K\rremote: Counting objects:  78% (77/98)\u001b[K\rremote: Counting objects:  79% (78/98)\u001b[K\rremote: Counting objects:  80% (79/98)\u001b[K\rremote: Counting objects:  81% (80/98)\u001b[K\rremote: Counting objects:  82% (81/98)\u001b[K\rremote: Counting objects:  83% (82/98)\u001b[K\rremote: Counting objects:  84% (83/98)\u001b[K\rremote: Counting objects:  85% (84/98)\u001b[K\rremote: Counting objects:  86% (85/98)\u001b[K\rremote: Counting objects:  87% (86/98)\u001b[K\rremote: Counting objects:  88% (87/98)\u001b[K\rremote: Counting objects:  89% (88/98)\u001b[K\rremote: Counting objects:  90% (89/98)\u001b[K\rremote: Counting objects:  91% (90/98)\u001b[K\rremote: Counting objects:  92% (91/98)\u001b[K\rremote: Counting objects:  93% (92/98)\u001b[K\rremote: Counting objects:  94% (93/98)\u001b[K\rremote: Counting objects:  95% (94/98)\u001b[K\rremote: Counting objects:  96% (95/98)\u001b[K\rremote: Counting objects:  97% (96/98)\u001b[K\rremote: Counting objects:  98% (97/98)\u001b[K\rremote: Counting objects: 100% (98/98)\u001b[K\rremote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 98 (delta 47), reused 72 (delta 23), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (98/98), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG7zgFHRYaiK",
        "colab_type": "code",
        "outputId": "e964e3a4-0b82-4865-ea9e-cad790d1e3fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7dOt4l5YfSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gm6vOSvY1JA",
        "colab_type": "code",
        "outputId": "6a62075d-5295-451e-9f45-ae0fe5385831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd tmp"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyqB6sTC_aRC",
        "colab_type": "code",
        "outputId": "0c460962-1c6a-4c93-f05e-3b0b732ee558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!mkdir rakdnngit\n",
        "!ls -la\n",
        "!cd /content/rakdnngit"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 12\n",
            "drwxr-xr-x 3 root root 4096 Jan  5 19:27 .\n",
            "drwxr-xr-x 1 root root 4096 Jan  5 19:27 ..\n",
            "drwxr-xr-x 2 root root 4096 Jan  5 19:27 rakdnngit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqgS8aH1X4ds",
        "colab_type": "code",
        "outputId": "bbb94621-56ea-4419-a825-84b03c69660c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/tmp"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FJhvKAE77Sx",
        "colab_type": "code",
        "outputId": "5569a0fe-8cfa-4ba0-d037-0fdf63a38414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF2wQjBiXGbD",
        "colab_type": "code",
        "outputId": "30f4f922-ef92-4bca-ae46-4066cdc79665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!cd /content/tmp\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rakdnngit\n",
            "/content/tmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEdtqOXG9h_t",
        "colab_type": "code",
        "outputId": "a49f7644-4fa4-4d44-ea2c-5d705c6070ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/rakdnngit"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/rakdnngit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVXfziEm8J_o",
        "colab_type": "code",
        "outputId": "c001c30d-a063-4285-d722-504db539da4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 7356\n",
            "drwxr-xr-x 5 root root    4096 Jan  5 19:27 .\n",
            "drwxr-xr-x 1 root root    4096 Jan  5 19:27 ..\n",
            "-rw-r--r-- 1 root root 7410460 Jan  5 19:27 dagmymodel.model\n",
            "drwxr-xr-x 8 root root    4096 Jan  5 19:27 .git\n",
            "drwxr-xr-x 2 root root    4096 Jan  5 19:27 localdata\n",
            "-rw-r--r-- 1 root root   85940 Jan  5 19:27 main.ipynb\n",
            "-rw-r--r-- 1 root root     513 Jan  5 19:27 main.py\n",
            "-rw-r--r-- 1 root root     178 Jan  5 19:27 requirement.txt\n",
            "-rw-r--r-- 1 root root     245 Jan  5 19:27 run_local\n",
            "drwxr-xr-x 2 root root    4096 Jan  5 19:27 trainer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD7NqpKR8O3T",
        "colab_type": "code",
        "outputId": "85da5cc9-ed52-4c9f-94e8-0a01f9c6bd51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "!pip3 install -r requirement.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.17.4 in /usr/local/lib/python3.6/dist-packages (from -r requirement.txt (line 1)) (1.17.4)\n",
            "Requirement already satisfied: Keras==2.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirement.txt (line 2)) (2.3.1)\n",
            "Requirement already satisfied: Keras-Applications==1.0.8 in /usr/local/lib/python3.6/dist-packages (from -r requirement.txt (line 3)) (1.0.8)\n",
            "Requirement already satisfied: Keras-Preprocessing==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirement.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: tensorboard==1.15.0 in /usr/local/lib/python3.6/dist-packages (from -r requirement.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: tensorflow==1.15.0 in /usr/local/lib/python3.6/dist-packages (from -r requirement.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from -r requirement.txt (line 7)) (1.15.1)\n",
            "Requirement already satisfied: tensorflow-tensorboard==0.1.8 in /usr/local/lib/python3.6/dist-packages (from -r requirement.txt (line 8)) (0.1.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.3.1->-r requirement.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.3.1->-r requirement.txt (line 2)) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.3.1->-r requirement.txt (line 2)) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.3.1->-r requirement.txt (line 2)) (1.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==1.15.0->-r requirement.txt (line 5)) (3.1.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==1.15.0->-r requirement.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==1.15.0->-r requirement.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==1.15.0->-r requirement.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==1.15.0->-r requirement.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==1.15.0->-r requirement.txt (line 5)) (0.33.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==1.15.0->-r requirement.txt (line 5)) (42.0.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->-r requirement.txt (line 6)) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->-r requirement.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->-r requirement.txt (line 6)) (1.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->-r requirement.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->-r requirement.txt (line 6)) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->-r requirement.txt (line 6)) (0.1.8)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard==0.1.8->-r requirement.txt (line 8)) (1.5.0)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard==0.1.8->-r requirement.txt (line 8)) (0.9999999)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgX2P3508nIL",
        "colab_type": "code",
        "outputId": "e8c4daf0-d58a-4fb5-d433-971ca82a8f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 main.py"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "number of sequences= 141 \n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "2020-01-05 19:28:24.948948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-01-05 19:28:24.962429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-05 19:28:24.962954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-01-05 19:28:24.963176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-01-05 19:28:24.964617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-01-05 19:28:24.966182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-01-05 19:28:24.966621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-01-05 19:28:24.968394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-01-05 19:28:24.969073: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-01-05 19:28:24.993750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-01-05 19:28:24.993885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-05 19:28:24.994524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-05 19:28:24.995047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-01-05 19:28:24.995395: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2020-01-05 19:28:25.010346: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz\n",
            "2020-01-05 19:28:25.010517: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1306bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-01-05 19:28:25.010547: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-01-05 19:28:25.098301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-05 19:28:25.099003: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1306d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-01-05 19:28:25.099034: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-01-05 19:28:25.099244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-05 19:28:25.099788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-01-05 19:28:25.099840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-01-05 19:28:25.099858: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-01-05 19:28:25.099871: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-01-05 19:28:25.099885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-01-05 19:28:25.099898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-01-05 19:28:25.099921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-01-05 19:28:25.099934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-01-05 19:28:25.099983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-05 19:28:25.100539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-05 19:28:25.101016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-01-05 19:28:25.101080: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-01-05 19:28:25.102088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-01-05 19:28:25.102113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-01-05 19:28:25.102122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-01-05 19:28:25.102207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-05 19:28:25.102775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-05 19:28:25.103383: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-01-05 19:28:25.103437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/500\n",
            "2020-01-05 19:28:26.229662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "141/141 [==============================] - 1s 5ms/step - loss: 4.4646\n",
            "Epoch 2/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 4.4347\n",
            "Epoch 3/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 4.3356\n",
            "Epoch 4/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 4.1713\n",
            "Epoch 5/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 4.0851\n",
            "Epoch 6/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 4.0407\n",
            "Epoch 7/500\n",
            "141/141 [==============================] - 0s 241us/step - loss: 3.9971\n",
            "Epoch 8/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 3.9477\n",
            "Epoch 9/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 3.8966\n",
            "Epoch 10/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 3.8447\n",
            "Epoch 11/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 3.8054\n",
            "Epoch 12/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 3.7456\n",
            "Epoch 13/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 3.7136\n",
            "Epoch 14/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 3.6841\n",
            "Epoch 15/500\n",
            "141/141 [==============================] - 0s 251us/step - loss: 3.5988\n",
            "Epoch 16/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 3.5467\n",
            "Epoch 17/500\n",
            "141/141 [==============================] - 0s 240us/step - loss: 3.4622\n",
            "Epoch 18/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 3.4128\n",
            "Epoch 19/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 3.3172\n",
            "Epoch 20/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 3.2419\n",
            "Epoch 21/500\n",
            "141/141 [==============================] - 0s 253us/step - loss: 3.1855\n",
            "Epoch 22/500\n",
            "141/141 [==============================] - 0s 284us/step - loss: 3.0983\n",
            "Epoch 23/500\n",
            "141/141 [==============================] - 0s 265us/step - loss: 3.0528\n",
            "Epoch 24/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 2.9617\n",
            "Epoch 25/500\n",
            "141/141 [==============================] - 0s 276us/step - loss: 2.9009\n",
            "Epoch 26/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 2.8239\n",
            "Epoch 27/500\n",
            "141/141 [==============================] - 0s 266us/step - loss: 2.7768\n",
            "Epoch 28/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 2.6884\n",
            "Epoch 29/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 2.5938\n",
            "Epoch 30/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 2.5189\n",
            "Epoch 31/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 2.4445\n",
            "Epoch 32/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 2.3233\n",
            "Epoch 33/500\n",
            "141/141 [==============================] - 0s 259us/step - loss: 2.2774\n",
            "Epoch 34/500\n",
            "141/141 [==============================] - 0s 235us/step - loss: 2.1700\n",
            "Epoch 35/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 2.0800\n",
            "Epoch 36/500\n",
            "141/141 [==============================] - 0s 261us/step - loss: 1.9973\n",
            "Epoch 37/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 1.9127\n",
            "Epoch 38/500\n",
            "141/141 [==============================] - 0s 244us/step - loss: 1.8552\n",
            "Epoch 39/500\n",
            "141/141 [==============================] - 0s 247us/step - loss: 1.6927\n",
            "Epoch 40/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 1.6573\n",
            "Epoch 41/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 1.5093\n",
            "Epoch 42/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 1.4624\n",
            "Epoch 43/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 1.3732\n",
            "Epoch 44/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 1.3146\n",
            "Epoch 45/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 1.2308\n",
            "Epoch 46/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 1.1475\n",
            "Epoch 47/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 1.1391\n",
            "Epoch 48/500\n",
            "141/141 [==============================] - 0s 256us/step - loss: 1.0712\n",
            "Epoch 49/500\n",
            "141/141 [==============================] - 0s 222us/step - loss: 0.9964\n",
            "Epoch 50/500\n",
            "141/141 [==============================] - 0s 215us/step - loss: 0.9196\n",
            "Epoch 51/500\n",
            "141/141 [==============================] - 0s 218us/step - loss: 0.8938\n",
            "Epoch 52/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 0.8704\n",
            "Epoch 53/500\n",
            "141/141 [==============================] - 0s 276us/step - loss: 0.7940\n",
            "Epoch 54/500\n",
            "141/141 [==============================] - 0s 280us/step - loss: 0.7600\n",
            "Epoch 55/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.6921\n",
            "Epoch 56/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.6377\n",
            "Epoch 57/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 0.6251\n",
            "Epoch 58/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 0.5862\n",
            "Epoch 59/500\n",
            "141/141 [==============================] - 0s 215us/step - loss: 0.5545\n",
            "Epoch 60/500\n",
            "141/141 [==============================] - 0s 217us/step - loss: 0.5584\n",
            "Epoch 61/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.5030\n",
            "Epoch 62/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.4608\n",
            "Epoch 63/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.4340\n",
            "Epoch 64/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.4301\n",
            "Epoch 65/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.3821\n",
            "Epoch 66/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.3849\n",
            "Epoch 67/500\n",
            "141/141 [==============================] - 0s 244us/step - loss: 0.3276\n",
            "Epoch 68/500\n",
            "141/141 [==============================] - 0s 222us/step - loss: 0.3287\n",
            "Epoch 69/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.3502\n",
            "Epoch 70/500\n",
            "141/141 [==============================] - 0s 246us/step - loss: 0.2711\n",
            "Epoch 71/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.2980\n",
            "Epoch 72/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.2780\n",
            "Epoch 73/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.2661\n",
            "Epoch 74/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 0.2257\n",
            "Epoch 75/500\n",
            "141/141 [==============================] - 0s 213us/step - loss: 0.2175\n",
            "Epoch 76/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.2102\n",
            "Epoch 77/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 0.2192\n",
            "Epoch 78/500\n",
            "141/141 [==============================] - 0s 219us/step - loss: 0.2058\n",
            "Epoch 79/500\n",
            "141/141 [==============================] - 0s 237us/step - loss: 0.1893\n",
            "Epoch 80/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.1646\n",
            "Epoch 81/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.1468\n",
            "Epoch 82/500\n",
            "141/141 [==============================] - 0s 246us/step - loss: 0.1544\n",
            "Epoch 83/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 0.1397\n",
            "Epoch 84/500\n",
            "141/141 [==============================] - 0s 275us/step - loss: 0.1323\n",
            "Epoch 85/500\n",
            "141/141 [==============================] - 0s 240us/step - loss: 0.1101\n",
            "Epoch 86/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.1246\n",
            "Epoch 87/500\n",
            "141/141 [==============================] - 0s 248us/step - loss: 0.1249\n",
            "Epoch 88/500\n",
            "141/141 [==============================] - 0s 252us/step - loss: 0.1104\n",
            "Epoch 89/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.1131\n",
            "Epoch 90/500\n",
            "141/141 [==============================] - 0s 218us/step - loss: 0.0969\n",
            "Epoch 91/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.1014\n",
            "Epoch 92/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0939\n",
            "Epoch 93/500\n",
            "141/141 [==============================] - 0s 219us/step - loss: 0.0909\n",
            "Epoch 94/500\n",
            "141/141 [==============================] - 0s 265us/step - loss: 0.1036\n",
            "Epoch 95/500\n",
            "141/141 [==============================] - 0s 217us/step - loss: 0.1033\n",
            "Epoch 96/500\n",
            "141/141 [==============================] - 0s 247us/step - loss: 0.0729\n",
            "Epoch 97/500\n",
            "141/141 [==============================] - 0s 266us/step - loss: 0.0682\n",
            "Epoch 98/500\n",
            "141/141 [==============================] - 0s 238us/step - loss: 0.0727\n",
            "Epoch 99/500\n",
            "141/141 [==============================] - 0s 239us/step - loss: 0.0561\n",
            "Epoch 100/500\n",
            "141/141 [==============================] - 0s 246us/step - loss: 0.0538\n",
            "Epoch 101/500\n",
            "141/141 [==============================] - 0s 243us/step - loss: 0.0625\n",
            "Epoch 102/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 0.0562\n",
            "Epoch 103/500\n",
            "141/141 [==============================] - 0s 251us/step - loss: 0.0810\n",
            "Epoch 104/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0477\n",
            "Epoch 105/500\n",
            "141/141 [==============================] - 0s 217us/step - loss: 0.0651\n",
            "Epoch 106/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0567\n",
            "Epoch 107/500\n",
            "141/141 [==============================] - 0s 219us/step - loss: 0.0761\n",
            "Epoch 108/500\n",
            "141/141 [==============================] - 0s 216us/step - loss: 0.0454\n",
            "Epoch 109/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.0481\n",
            "Epoch 110/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 0.0407\n",
            "Epoch 111/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 0.0364\n",
            "Epoch 112/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0507\n",
            "Epoch 113/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0317\n",
            "Epoch 114/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 0.0473\n",
            "Epoch 115/500\n",
            "141/141 [==============================] - 0s 339us/step - loss: 0.0523\n",
            "Epoch 116/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 0.0517\n",
            "Epoch 117/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0581\n",
            "Epoch 118/500\n",
            "141/141 [==============================] - 0s 253us/step - loss: 0.0337\n",
            "Epoch 119/500\n",
            "141/141 [==============================] - 0s 241us/step - loss: 0.0353\n",
            "Epoch 120/500\n",
            "141/141 [==============================] - 0s 239us/step - loss: 0.0312\n",
            "Epoch 121/500\n",
            "141/141 [==============================] - 0s 250us/step - loss: 0.0279\n",
            "Epoch 122/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0355\n",
            "Epoch 123/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0333\n",
            "Epoch 124/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 0.0251\n",
            "Epoch 125/500\n",
            "141/141 [==============================] - 0s 245us/step - loss: 0.0407\n",
            "Epoch 126/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0273\n",
            "Epoch 127/500\n",
            "141/141 [==============================] - 0s 293us/step - loss: 0.0354\n",
            "Epoch 128/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0211\n",
            "Epoch 129/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0307\n",
            "Epoch 130/500\n",
            "141/141 [==============================] - 0s 282us/step - loss: 0.0359\n",
            "Epoch 131/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0261\n",
            "Epoch 132/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.0238\n",
            "Epoch 133/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0307\n",
            "Epoch 134/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0304\n",
            "Epoch 135/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0186\n",
            "Epoch 136/500\n",
            "141/141 [==============================] - 0s 217us/step - loss: 0.0222\n",
            "Epoch 137/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0163\n",
            "Epoch 138/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0304\n",
            "Epoch 139/500\n",
            "141/141 [==============================] - 0s 261us/step - loss: 0.0499\n",
            "Epoch 140/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0440\n",
            "Epoch 141/500\n",
            "141/141 [==============================] - 0s 217us/step - loss: 0.0187\n",
            "Epoch 142/500\n",
            "141/141 [==============================] - 0s 248us/step - loss: 0.0163\n",
            "Epoch 143/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0188\n",
            "Epoch 144/500\n",
            "141/141 [==============================] - 0s 256us/step - loss: 0.0216\n",
            "Epoch 145/500\n",
            "141/141 [==============================] - 0s 315us/step - loss: 0.0315\n",
            "Epoch 146/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0325\n",
            "Epoch 147/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 0.0180\n",
            "Epoch 148/500\n",
            "141/141 [==============================] - 0s 239us/step - loss: 0.0092\n",
            "Epoch 149/500\n",
            "141/141 [==============================] - 0s 222us/step - loss: 0.0190\n",
            "Epoch 150/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.0135\n",
            "Epoch 151/500\n",
            "141/141 [==============================] - 0s 222us/step - loss: 0.0097\n",
            "Epoch 152/500\n",
            "141/141 [==============================] - 0s 217us/step - loss: 0.0159\n",
            "Epoch 153/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0286\n",
            "Epoch 154/500\n",
            "141/141 [==============================] - 0s 239us/step - loss: 0.0093\n",
            "Epoch 155/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0102\n",
            "Epoch 156/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0160\n",
            "Epoch 157/500\n",
            "141/141 [==============================] - 0s 246us/step - loss: 0.0234\n",
            "Epoch 158/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0180\n",
            "Epoch 159/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0195\n",
            "Epoch 160/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0148\n",
            "Epoch 161/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0175\n",
            "Epoch 162/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.0118\n",
            "Epoch 163/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0150\n",
            "Epoch 164/500\n",
            "141/141 [==============================] - 0s 251us/step - loss: 0.0221\n",
            "Epoch 165/500\n",
            "141/141 [==============================] - 0s 248us/step - loss: 0.0065\n",
            "Epoch 166/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 0.0123\n",
            "Epoch 167/500\n",
            "141/141 [==============================] - 0s 235us/step - loss: 0.0142\n",
            "Epoch 168/500\n",
            "141/141 [==============================] - 0s 238us/step - loss: 0.0085\n",
            "Epoch 169/500\n",
            "141/141 [==============================] - 0s 245us/step - loss: 0.0120\n",
            "Epoch 170/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.0265\n",
            "Epoch 171/500\n",
            "141/141 [==============================] - 0s 235us/step - loss: 0.0083\n",
            "Epoch 172/500\n",
            "141/141 [==============================] - 0s 271us/step - loss: 0.0077\n",
            "Epoch 173/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 0.0107\n",
            "Epoch 174/500\n",
            "141/141 [==============================] - 0s 262us/step - loss: 0.0139\n",
            "Epoch 175/500\n",
            "141/141 [==============================] - 0s 301us/step - loss: 0.0181\n",
            "Epoch 176/500\n",
            "141/141 [==============================] - 0s 302us/step - loss: 0.0154\n",
            "Epoch 177/500\n",
            "141/141 [==============================] - 0s 235us/step - loss: 0.0204\n",
            "Epoch 178/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0153\n",
            "Epoch 179/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0173\n",
            "Epoch 180/500\n",
            "141/141 [==============================] - 0s 246us/step - loss: 0.0090\n",
            "Epoch 181/500\n",
            "141/141 [==============================] - 0s 268us/step - loss: 0.0083\n",
            "Epoch 182/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0176\n",
            "Epoch 183/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 0.0094\n",
            "Epoch 184/500\n",
            "141/141 [==============================] - 0s 244us/step - loss: 0.0091\n",
            "Epoch 185/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 0.0130\n",
            "Epoch 186/500\n",
            "141/141 [==============================] - 0s 245us/step - loss: 0.0164\n",
            "Epoch 187/500\n",
            "141/141 [==============================] - 0s 261us/step - loss: 0.0066\n",
            "Epoch 188/500\n",
            "141/141 [==============================] - 0s 254us/step - loss: 0.0227\n",
            "Epoch 189/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0121\n",
            "Epoch 190/500\n",
            "141/141 [==============================] - 0s 237us/step - loss: 0.0158\n",
            "Epoch 191/500\n",
            "141/141 [==============================] - 0s 235us/step - loss: 0.0173\n",
            "Epoch 192/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0087\n",
            "Epoch 193/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 0.0071\n",
            "Epoch 194/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0073\n",
            "Epoch 195/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0100\n",
            "Epoch 196/500\n",
            "141/141 [==============================] - 0s 267us/step - loss: 0.0116\n",
            "Epoch 197/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0138\n",
            "Epoch 198/500\n",
            "141/141 [==============================] - 0s 248us/step - loss: 0.0220\n",
            "Epoch 199/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0177\n",
            "Epoch 200/500\n",
            "141/141 [==============================] - 0s 250us/step - loss: 0.0145\n",
            "Epoch 201/500\n",
            "141/141 [==============================] - 0s 243us/step - loss: 0.0178\n",
            "Epoch 202/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 0.0109\n",
            "Epoch 203/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0172\n",
            "Epoch 204/500\n",
            "141/141 [==============================] - 0s 246us/step - loss: 0.0141\n",
            "Epoch 205/500\n",
            "141/141 [==============================] - 0s 296us/step - loss: 0.0226\n",
            "Epoch 206/500\n",
            "141/141 [==============================] - 0s 241us/step - loss: 0.0098\n",
            "Epoch 207/500\n",
            "141/141 [==============================] - 0s 238us/step - loss: 0.0189\n",
            "Epoch 208/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0099\n",
            "Epoch 209/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0199\n",
            "Epoch 210/500\n",
            "141/141 [==============================] - 0s 238us/step - loss: 0.0086\n",
            "Epoch 211/500\n",
            "141/141 [==============================] - 0s 240us/step - loss: 0.0139\n",
            "Epoch 212/500\n",
            "141/141 [==============================] - 0s 244us/step - loss: 0.0094\n",
            "Epoch 213/500\n",
            "141/141 [==============================] - 0s 259us/step - loss: 0.0057\n",
            "Epoch 214/500\n",
            "141/141 [==============================] - 0s 243us/step - loss: 0.0099\n",
            "Epoch 215/500\n",
            "141/141 [==============================] - 0s 240us/step - loss: 0.0132\n",
            "Epoch 216/500\n",
            "141/141 [==============================] - 0s 248us/step - loss: 0.0055\n",
            "Epoch 217/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0089\n",
            "Epoch 218/500\n",
            "141/141 [==============================] - 0s 265us/step - loss: 0.0112\n",
            "Epoch 219/500\n",
            "141/141 [==============================] - 0s 250us/step - loss: 0.0133\n",
            "Epoch 220/500\n",
            "141/141 [==============================] - 0s 239us/step - loss: 0.0134\n",
            "Epoch 221/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 0.0143\n",
            "Epoch 222/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 0.0084\n",
            "Epoch 223/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0116\n",
            "Epoch 224/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.0079\n",
            "Epoch 225/500\n",
            "141/141 [==============================] - 0s 239us/step - loss: 0.0120\n",
            "Epoch 226/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.0166\n",
            "Epoch 227/500\n",
            "141/141 [==============================] - 0s 258us/step - loss: 0.0091\n",
            "Epoch 228/500\n",
            "141/141 [==============================] - 0s 307us/step - loss: 0.0125\n",
            "Epoch 229/500\n",
            "141/141 [==============================] - 0s 253us/step - loss: 0.0147\n",
            "Epoch 230/500\n",
            "141/141 [==============================] - 0s 247us/step - loss: 0.0162\n",
            "Epoch 231/500\n",
            "141/141 [==============================] - 0s 273us/step - loss: 0.0071\n",
            "Epoch 232/500\n",
            "141/141 [==============================] - 0s 250us/step - loss: 0.0225\n",
            "Epoch 233/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.0061\n",
            "Epoch 234/500\n",
            "141/141 [==============================] - 0s 276us/step - loss: 0.0119\n",
            "Epoch 235/500\n",
            "141/141 [==============================] - 0s 256us/step - loss: 0.0136\n",
            "Epoch 236/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0099\n",
            "Epoch 237/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0085\n",
            "Epoch 238/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0146\n",
            "Epoch 239/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.0122\n",
            "Epoch 240/500\n",
            "141/141 [==============================] - 0s 256us/step - loss: 0.0132\n",
            "Epoch 241/500\n",
            "141/141 [==============================] - 0s 237us/step - loss: 0.0112\n",
            "Epoch 242/500\n",
            "141/141 [==============================] - 0s 240us/step - loss: 0.0154\n",
            "Epoch 243/500\n",
            "141/141 [==============================] - 0s 239us/step - loss: 0.0085\n",
            "Epoch 244/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 0.0118\n",
            "Epoch 245/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0155\n",
            "Epoch 246/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.0114\n",
            "Epoch 247/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 0.0103\n",
            "Epoch 248/500\n",
            "141/141 [==============================] - 0s 253us/step - loss: 0.0103\n",
            "Epoch 249/500\n",
            "141/141 [==============================] - 0s 237us/step - loss: 0.0131\n",
            "Epoch 250/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0204\n",
            "Epoch 251/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0130\n",
            "Epoch 252/500\n",
            "141/141 [==============================] - 0s 213us/step - loss: 0.0079\n",
            "Epoch 253/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0093\n",
            "Epoch 254/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.0090\n",
            "Epoch 255/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0068\n",
            "Epoch 256/500\n",
            "141/141 [==============================] - 0s 235us/step - loss: 0.0162\n",
            "Epoch 257/500\n",
            "141/141 [==============================] - 0s 247us/step - loss: 0.0117\n",
            "Epoch 258/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 0.0094\n",
            "Epoch 259/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.0156\n",
            "Epoch 260/500\n",
            "141/141 [==============================] - 0s 238us/step - loss: 0.0131\n",
            "Epoch 261/500\n",
            "141/141 [==============================] - 0s 243us/step - loss: 0.0089\n",
            "Epoch 262/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0082\n",
            "Epoch 263/500\n",
            "141/141 [==============================] - 0s 256us/step - loss: 0.0257\n",
            "Epoch 264/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0187\n",
            "Epoch 265/500\n",
            "141/141 [==============================] - 0s 277us/step - loss: 0.0059\n",
            "Epoch 266/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0111\n",
            "Epoch 267/500\n",
            "141/141 [==============================] - 0s 258us/step - loss: 0.0166\n",
            "Epoch 268/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0094\n",
            "Epoch 269/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 0.0078\n",
            "Epoch 270/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0072\n",
            "Epoch 271/500\n",
            "141/141 [==============================] - 0s 238us/step - loss: 0.0128\n",
            "Epoch 272/500\n",
            "141/141 [==============================] - 0s 264us/step - loss: 0.0153\n",
            "Epoch 273/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 0.0039\n",
            "Epoch 274/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.0103\n",
            "Epoch 275/500\n",
            "141/141 [==============================] - 0s 243us/step - loss: 0.0221\n",
            "Epoch 276/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0136\n",
            "Epoch 277/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.0113\n",
            "Epoch 278/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0127\n",
            "Epoch 279/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0167\n",
            "Epoch 280/500\n",
            "141/141 [==============================] - 0s 243us/step - loss: 0.0210\n",
            "Epoch 281/500\n",
            "141/141 [==============================] - 0s 259us/step - loss: 0.0189\n",
            "Epoch 282/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.0042\n",
            "Epoch 283/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.0146\n",
            "Epoch 284/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0178\n",
            "Epoch 285/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0203\n",
            "Epoch 286/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0250\n",
            "Epoch 287/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0088\n",
            "Epoch 288/500\n",
            "141/141 [==============================] - 0s 238us/step - loss: 0.0166\n",
            "Epoch 289/500\n",
            "141/141 [==============================] - 0s 244us/step - loss: 0.0144\n",
            "Epoch 290/500\n",
            "141/141 [==============================] - 0s 254us/step - loss: 0.0136\n",
            "Epoch 291/500\n",
            "141/141 [==============================] - 0s 251us/step - loss: 0.0116\n",
            "Epoch 292/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.0139\n",
            "Epoch 293/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.0171\n",
            "Epoch 294/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0078\n",
            "Epoch 295/500\n",
            "141/141 [==============================] - 0s 251us/step - loss: 0.0183\n",
            "Epoch 296/500\n",
            "141/141 [==============================] - 0s 268us/step - loss: 0.0169\n",
            "Epoch 297/500\n",
            "141/141 [==============================] - 0s 252us/step - loss: 0.0083\n",
            "Epoch 298/500\n",
            "141/141 [==============================] - 0s 241us/step - loss: 0.0094\n",
            "Epoch 299/500\n",
            "141/141 [==============================] - 0s 255us/step - loss: 0.0123\n",
            "Epoch 300/500\n",
            "141/141 [==============================] - 0s 255us/step - loss: 0.0114\n",
            "Epoch 301/500\n",
            "141/141 [==============================] - 0s 237us/step - loss: 0.0134\n",
            "Epoch 302/500\n",
            "141/141 [==============================] - 0s 239us/step - loss: 0.0149\n",
            "Epoch 303/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0145\n",
            "Epoch 304/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0182\n",
            "Epoch 305/500\n",
            "141/141 [==============================] - 0s 264us/step - loss: 0.0134\n",
            "Epoch 306/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0106\n",
            "Epoch 307/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0063\n",
            "Epoch 308/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0090\n",
            "Epoch 309/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0172\n",
            "Epoch 310/500\n",
            "141/141 [==============================] - 0s 222us/step - loss: 0.0074\n",
            "Epoch 311/500\n",
            "141/141 [==============================] - 0s 257us/step - loss: 0.0118\n",
            "Epoch 312/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 0.0155\n",
            "Epoch 313/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.0308\n",
            "Epoch 314/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0127\n",
            "Epoch 315/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.0042\n",
            "Epoch 316/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0204\n",
            "Epoch 317/500\n",
            "141/141 [==============================] - 0s 235us/step - loss: 0.0178\n",
            "Epoch 318/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0103\n",
            "Epoch 319/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0246\n",
            "Epoch 320/500\n",
            "141/141 [==============================] - 0s 271us/step - loss: 0.0075\n",
            "Epoch 321/500\n",
            "141/141 [==============================] - 0s 259us/step - loss: 0.0143\n",
            "Epoch 322/500\n",
            "141/141 [==============================] - 0s 240us/step - loss: 0.0126\n",
            "Epoch 323/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0105\n",
            "Epoch 324/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0084\n",
            "Epoch 325/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0129\n",
            "Epoch 326/500\n",
            "141/141 [==============================] - 0s 332us/step - loss: 0.0073\n",
            "Epoch 327/500\n",
            "141/141 [==============================] - 0s 240us/step - loss: 0.0075\n",
            "Epoch 328/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 0.0129\n",
            "Epoch 329/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 0.0104\n",
            "Epoch 330/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0063\n",
            "Epoch 331/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0093\n",
            "Epoch 332/500\n",
            "141/141 [==============================] - 0s 275us/step - loss: 0.0120\n",
            "Epoch 333/500\n",
            "141/141 [==============================] - 0s 237us/step - loss: 0.0136\n",
            "Epoch 334/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0103\n",
            "Epoch 335/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0078\n",
            "Epoch 336/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.0114\n",
            "Epoch 337/500\n",
            "141/141 [==============================] - 0s 240us/step - loss: 0.0193\n",
            "Epoch 338/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0241\n",
            "Epoch 339/500\n",
            "141/141 [==============================] - 0s 222us/step - loss: 0.0101\n",
            "Epoch 340/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0149\n",
            "Epoch 341/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0105\n",
            "Epoch 342/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.0196\n",
            "Epoch 343/500\n",
            "141/141 [==============================] - 0s 219us/step - loss: 0.0131\n",
            "Epoch 344/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0128\n",
            "Epoch 345/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0219\n",
            "Epoch 346/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.0163\n",
            "Epoch 347/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 0.0061\n",
            "Epoch 348/500\n",
            "141/141 [==============================] - 0s 218us/step - loss: 0.0130\n",
            "Epoch 349/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0139\n",
            "Epoch 350/500\n",
            "141/141 [==============================] - 0s 258us/step - loss: 0.0103\n",
            "Epoch 351/500\n",
            "141/141 [==============================] - 0s 239us/step - loss: 0.0093\n",
            "Epoch 352/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 0.0157\n",
            "Epoch 353/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0161\n",
            "Epoch 354/500\n",
            "141/141 [==============================] - 0s 213us/step - loss: 0.0135\n",
            "Epoch 355/500\n",
            "141/141 [==============================] - 0s 214us/step - loss: 0.0078\n",
            "Epoch 356/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 0.0118\n",
            "Epoch 357/500\n",
            "141/141 [==============================] - 0s 301us/step - loss: 0.0173\n",
            "Epoch 358/500\n",
            "141/141 [==============================] - 0s 217us/step - loss: 0.0100\n",
            "Epoch 359/500\n",
            "141/141 [==============================] - 0s 219us/step - loss: 0.0123\n",
            "Epoch 360/500\n",
            "141/141 [==============================] - 0s 237us/step - loss: 0.0056\n",
            "Epoch 361/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0085\n",
            "Epoch 362/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0134\n",
            "Epoch 363/500\n",
            "141/141 [==============================] - 0s 241us/step - loss: 0.0079\n",
            "Epoch 364/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0127\n",
            "Epoch 365/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0145\n",
            "Epoch 366/500\n",
            "141/141 [==============================] - 0s 279us/step - loss: 0.0118\n",
            "Epoch 367/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0146\n",
            "Epoch 368/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.0126\n",
            "Epoch 369/500\n",
            "141/141 [==============================] - 0s 250us/step - loss: 0.0191\n",
            "Epoch 370/500\n",
            "141/141 [==============================] - 0s 243us/step - loss: 0.0135\n",
            "Epoch 371/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0126\n",
            "Epoch 372/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 0.0109\n",
            "Epoch 373/500\n",
            "141/141 [==============================] - 0s 218us/step - loss: 0.0116\n",
            "Epoch 374/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0179\n",
            "Epoch 375/500\n",
            "141/141 [==============================] - 0s 264us/step - loss: 0.0119\n",
            "Epoch 376/500\n",
            "141/141 [==============================] - 0s 243us/step - loss: 0.0122\n",
            "Epoch 377/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.0184\n",
            "Epoch 378/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0135\n",
            "Epoch 379/500\n",
            "141/141 [==============================] - 0s 213us/step - loss: 0.0127\n",
            "Epoch 380/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0113\n",
            "Epoch 381/500\n",
            "141/141 [==============================] - 0s 265us/step - loss: 0.0066\n",
            "Epoch 382/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0163\n",
            "Epoch 383/500\n",
            "141/141 [==============================] - 0s 244us/step - loss: 0.0081\n",
            "Epoch 384/500\n",
            "141/141 [==============================] - 0s 252us/step - loss: 0.0108\n",
            "Epoch 385/500\n",
            "141/141 [==============================] - 0s 214us/step - loss: 0.0145\n",
            "Epoch 386/500\n",
            "141/141 [==============================] - 0s 212us/step - loss: 0.0074\n",
            "Epoch 387/500\n",
            "141/141 [==============================] - 0s 225us/step - loss: 0.0121\n",
            "Epoch 388/500\n",
            "141/141 [==============================] - 0s 276us/step - loss: 0.0274\n",
            "Epoch 389/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0139\n",
            "Epoch 390/500\n",
            "141/141 [==============================] - 0s 261us/step - loss: 0.0075\n",
            "Epoch 391/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0117\n",
            "Epoch 392/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 0.0130\n",
            "Epoch 393/500\n",
            "141/141 [==============================] - 0s 273us/step - loss: 0.0148\n",
            "Epoch 394/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0208\n",
            "Epoch 395/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0168\n",
            "Epoch 396/500\n",
            "141/141 [==============================] - 0s 243us/step - loss: 0.0139\n",
            "Epoch 397/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 0.0151\n",
            "Epoch 398/500\n",
            "141/141 [==============================] - 0s 252us/step - loss: 0.0100\n",
            "Epoch 399/500\n",
            "141/141 [==============================] - 0s 280us/step - loss: 0.0141\n",
            "Epoch 400/500\n",
            "141/141 [==============================] - 0s 247us/step - loss: 0.0184\n",
            "Epoch 401/500\n",
            "141/141 [==============================] - 0s 255us/step - loss: 0.0069\n",
            "Epoch 402/500\n",
            "141/141 [==============================] - 0s 240us/step - loss: 0.0087\n",
            "Epoch 403/500\n",
            "141/141 [==============================] - 0s 239us/step - loss: 0.0154\n",
            "Epoch 404/500\n",
            "141/141 [==============================] - 0s 267us/step - loss: 0.0133\n",
            "Epoch 405/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 0.0140\n",
            "Epoch 406/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.0146\n",
            "Epoch 407/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0155\n",
            "Epoch 408/500\n",
            "141/141 [==============================] - 0s 226us/step - loss: 0.0184\n",
            "Epoch 409/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0138\n",
            "Epoch 410/500\n",
            "141/141 [==============================] - 0s 266us/step - loss: 0.0099\n",
            "Epoch 411/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 0.0130\n",
            "Epoch 412/500\n",
            "141/141 [==============================] - 0s 214us/step - loss: 0.0107\n",
            "Epoch 413/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 0.0062\n",
            "Epoch 414/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0125\n",
            "Epoch 415/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 0.0063\n",
            "Epoch 416/500\n",
            "141/141 [==============================] - 0s 222us/step - loss: 0.0111\n",
            "Epoch 417/500\n",
            "141/141 [==============================] - 0s 287us/step - loss: 0.0073\n",
            "Epoch 418/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0088\n",
            "Epoch 419/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 0.0098\n",
            "Epoch 420/500\n",
            "141/141 [==============================] - 0s 219us/step - loss: 0.0097\n",
            "Epoch 421/500\n",
            "141/141 [==============================] - 0s 215us/step - loss: 0.0039\n",
            "Epoch 422/500\n",
            "141/141 [==============================] - 0s 247us/step - loss: 0.0104\n",
            "Epoch 423/500\n",
            "141/141 [==============================] - 0s 222us/step - loss: 0.0073\n",
            "Epoch 424/500\n",
            "141/141 [==============================] - 0s 219us/step - loss: 0.0087\n",
            "Epoch 425/500\n",
            "141/141 [==============================] - 0s 214us/step - loss: 0.0105\n",
            "Epoch 426/500\n",
            "141/141 [==============================] - 0s 241us/step - loss: 0.0096\n",
            "Epoch 427/500\n",
            "141/141 [==============================] - 0s 256us/step - loss: 0.0162\n",
            "Epoch 428/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 0.0123\n",
            "Epoch 429/500\n",
            "141/141 [==============================] - 0s 235us/step - loss: 0.0124\n",
            "Epoch 430/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0123\n",
            "Epoch 431/500\n",
            "141/141 [==============================] - 0s 237us/step - loss: 0.0103\n",
            "Epoch 432/500\n",
            "141/141 [==============================] - 0s 245us/step - loss: 0.0143\n",
            "Epoch 433/500\n",
            "141/141 [==============================] - 0s 241us/step - loss: 0.0114\n",
            "Epoch 434/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 0.0124\n",
            "Epoch 435/500\n",
            "141/141 [==============================] - 0s 282us/step - loss: 0.0127\n",
            "Epoch 436/500\n",
            "141/141 [==============================] - 0s 245us/step - loss: 0.0096\n",
            "Epoch 437/500\n",
            "141/141 [==============================] - 0s 251us/step - loss: 0.0117\n",
            "Epoch 438/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0086\n",
            "Epoch 439/500\n",
            "141/141 [==============================] - 0s 236us/step - loss: 0.0097\n",
            "Epoch 440/500\n",
            "141/141 [==============================] - 0s 235us/step - loss: 0.0116\n",
            "Epoch 441/500\n",
            "141/141 [==============================] - 0s 274us/step - loss: 0.0144\n",
            "Epoch 442/500\n",
            "141/141 [==============================] - 0s 234us/step - loss: 0.0135\n",
            "Epoch 443/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 0.0236\n",
            "Epoch 444/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0123\n",
            "Epoch 445/500\n",
            "141/141 [==============================] - 0s 215us/step - loss: 0.0079\n",
            "Epoch 446/500\n",
            "141/141 [==============================] - 0s 249us/step - loss: 0.0177\n",
            "Epoch 447/500\n",
            "141/141 [==============================] - 0s 218us/step - loss: 0.0115\n",
            "Epoch 448/500\n",
            "141/141 [==============================] - 0s 299us/step - loss: 0.0104\n",
            "Epoch 449/500\n",
            "141/141 [==============================] - 0s 257us/step - loss: 0.0130\n",
            "Epoch 450/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0114\n",
            "Epoch 451/500\n",
            "141/141 [==============================] - 0s 247us/step - loss: 0.0079\n",
            "Epoch 452/500\n",
            "141/141 [==============================] - 0s 253us/step - loss: 0.0136\n",
            "Epoch 453/500\n",
            "141/141 [==============================] - 0s 257us/step - loss: 0.0149\n",
            "Epoch 454/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0088\n",
            "Epoch 455/500\n",
            "141/141 [==============================] - 0s 238us/step - loss: 0.0068\n",
            "Epoch 456/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0115\n",
            "Epoch 457/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0113\n",
            "Epoch 458/500\n",
            "141/141 [==============================] - 0s 260us/step - loss: 0.0136\n",
            "Epoch 459/500\n",
            "141/141 [==============================] - 0s 223us/step - loss: 0.0132\n",
            "Epoch 460/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0115\n",
            "Epoch 461/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0248\n",
            "Epoch 462/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0151\n",
            "Epoch 463/500\n",
            "141/141 [==============================] - 0s 245us/step - loss: 0.0179\n",
            "Epoch 464/500\n",
            "141/141 [==============================] - 0s 252us/step - loss: 0.0103\n",
            "Epoch 465/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0075\n",
            "Epoch 466/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 0.0088\n",
            "Epoch 467/500\n",
            "141/141 [==============================] - 0s 233us/step - loss: 0.0084\n",
            "Epoch 468/500\n",
            "141/141 [==============================] - 0s 244us/step - loss: 0.0150\n",
            "Epoch 469/500\n",
            "141/141 [==============================] - 0s 229us/step - loss: 0.0127\n",
            "Epoch 470/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0090\n",
            "Epoch 471/500\n",
            "141/141 [==============================] - 0s 240us/step - loss: 0.0125\n",
            "Epoch 472/500\n",
            "141/141 [==============================] - 0s 242us/step - loss: 0.0226\n",
            "Epoch 473/500\n",
            "141/141 [==============================] - 0s 244us/step - loss: 0.0071\n",
            "Epoch 474/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0146\n",
            "Epoch 475/500\n",
            "141/141 [==============================] - 0s 228us/step - loss: 0.0138\n",
            "Epoch 476/500\n",
            "141/141 [==============================] - 0s 216us/step - loss: 0.0136\n",
            "Epoch 477/500\n",
            "141/141 [==============================] - 0s 244us/step - loss: 0.0127\n",
            "Epoch 478/500\n",
            "141/141 [==============================] - 0s 278us/step - loss: 0.0152\n",
            "Epoch 479/500\n",
            "141/141 [==============================] - 0s 272us/step - loss: 0.0136\n",
            "Epoch 480/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0084\n",
            "Epoch 481/500\n",
            "141/141 [==============================] - 0s 251us/step - loss: 0.0053\n",
            "Epoch 482/500\n",
            "141/141 [==============================] - 0s 230us/step - loss: 0.0053\n",
            "Epoch 483/500\n",
            "141/141 [==============================] - 0s 227us/step - loss: 0.0101\n",
            "Epoch 484/500\n",
            "141/141 [==============================] - 0s 220us/step - loss: 0.0106\n",
            "Epoch 485/500\n",
            "141/141 [==============================] - 0s 231us/step - loss: 0.0126\n",
            "Epoch 486/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0195\n",
            "Epoch 487/500\n",
            "141/141 [==============================] - 0s 219us/step - loss: 0.0095\n",
            "Epoch 488/500\n",
            "141/141 [==============================] - 0s 243us/step - loss: 0.0195\n",
            "Epoch 489/500\n",
            "141/141 [==============================] - 0s 219us/step - loss: 0.0222\n",
            "Epoch 490/500\n",
            "141/141 [==============================] - 0s 224us/step - loss: 0.0202\n",
            "Epoch 491/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0115\n",
            "Epoch 492/500\n",
            "141/141 [==============================] - 0s 217us/step - loss: 0.0088\n",
            "Epoch 493/500\n",
            "141/141 [==============================] - 0s 218us/step - loss: 0.0109\n",
            "Epoch 494/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0145\n",
            "Epoch 495/500\n",
            "141/141 [==============================] - 0s 232us/step - loss: 0.0130\n",
            "Epoch 496/500\n",
            "141/141 [==============================] - 0s 218us/step - loss: 0.0056\n",
            "Epoch 497/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0072\n",
            "Epoch 498/500\n",
            "141/141 [==============================] - 0s 248us/step - loss: 0.0077\n",
            "Epoch 499/500\n",
            "141/141 [==============================] - 0s 221us/step - loss: 0.0170\n",
            "Epoch 500/500\n",
            "141/141 [==============================] - 0s 245us/step - loss: 0.0095\n",
            "*************** training main - done\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, None, 100)         8700      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 256)         365568    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 87)                22359     \n",
            "=================================================================\n",
            "Total params: 921,939\n",
            "Trainable params: 921,939\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "output from loaded model : ( till handelsman och ) =>  ha\n",
            "*************** test main - done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztsafPFT9_ZW",
        "colab_type": "code",
        "outputId": "c059bcc2-965f-430a-badf-68ff7f69ea17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/rakdnngit'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruGCiYM9-Brw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "more localdata/hbshort.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THsPNQzRZlpW",
        "colab_type": "code",
        "outputId": "1bfebc05-88e4-4551-c455-637410976d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "ls -rla /content/tmp/rakdnngit/jobdir/rak_dnngit"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 7248\n",
            "-rw-r--r-- 1 root root 7410460 Jan  5 19:28 allincluded.model\n",
            "drwxr-xr-x 3 root root    4096 Jan  5 19:28 \u001b[0m\u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x 2 root root    4096 Jan  5 19:28 \u001b[01;34m.\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTfn8YtE7t9j",
        "colab_type": "code",
        "outputId": "a731b4ef-debc-4fb4-efa5-f735107776ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# *********************************************************************************************************************\n",
        "# Library imports\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input, Conv2D,LeakyReLU, Flatten, Dense,Reshape,Activation,Conv2DTranspose,Embedding,Dropout,LSTM\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils import np_utils\n",
        "import re\n",
        "import argparse\n",
        "# *********************************************************************************************************************\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8K1XHpg2Xxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ext_seq_length=4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvpS4dDx7wgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *********************************************************************************************************************\n",
        "# Load data from file\n",
        "def load_data(datapath):\n",
        "    filename=datapath+\"/hblang.txt\"\n",
        "    with open(filename,encoding=\"utf-8-sig\") as f:\n",
        "        text =f.read()\n",
        "\n",
        "    seq_length=ext_seq_length # sequence around sentence\n",
        "    step=1 # prediction step forward\n",
        "\n",
        "    start_story='| '*seq_length\n",
        "\n",
        "    text=text.lower()\n",
        "    text=start_story+text\n",
        "    text=text.replace('\\n\\n\\n\\n\\n',start_story)\n",
        "    text=text.replace('\\n',' ')\n",
        "    #text=re.sub(' +','. ',text).strip()\n",
        "    text=text.replace('..','.')\n",
        "    text=re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_{|}~])',r' \\1 ',text)\n",
        "    text=re.sub('\\s{2,}',' ',text)\n",
        "\n",
        "    tokenizer=Tokenizer(char_level=False,filters='')\n",
        "    tokenizer.fit_on_texts([text])\n",
        "    total_words=len(tokenizer.word_index)+1\n",
        "    token_list=tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "    return token_list, seq_length,step,total_words,start_story,tokenizer\n",
        "# *********************************************************************************************************************"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zr_pQdy73fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text2(seed_text,next_words,model,max_sequence_len,temp,start_story,tokenizer):\n",
        "    token_list = np.array(tokenizer.texts_to_sequences([seed_text])[0])\n",
        "    token_list=token_list[-ext_seq_length:]\n",
        "    probs = model.predict(token_list.reshape(1,len(token_list)), verbose=0)[0]\n",
        "    \n",
        "    print(1)\n",
        "    p0=-np.sort(-probs)\n",
        "    print(2)\n",
        "    i0=np.argsort(-probs)\n",
        "    print(3)\n",
        "    print(\"words probablility\",p0[0:9])\n",
        "    print(\"words index=\",i0[0:9])\n",
        "    print(4)\n",
        "    pos=0\n",
        "    for k in i0:\n",
        "      print(\"words in order=\",k,\" : \",tokenizer.index_word[k],'p=',p0[pos])\n",
        "      if pos>10:\n",
        "        break\n",
        "      pos+=1\n",
        "    print(5)\n",
        "    \n",
        "    y_class = sample_with_temp(probs, temperature=temp)\n",
        "    output_word = tokenizer.index_word[y_class] if y_class > 0 else ''\n",
        "    return output_word\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ6gzbtc-KWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *********************************************************************************************************************\n",
        "def sample_with_temp(preds,temperature=1.0):\n",
        "    preds=np.asarray(preds).astype('float64')\n",
        "    preds=np.log(preds)/temperature\n",
        "    exp_preds=np.exp(preds)\n",
        "    preds=exp_preds/np.sum(exp_preds)\n",
        "    probs=np.random.multinomial(1,preds,1)\n",
        "    return np.argmax(probs)\n",
        "# *********************************************************************************************************************\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNUWlFfj7-rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(args):\n",
        "    fileandpath = os.path.join(args.job_dir, 'rak_dnngit')+\"/allincluded.model\"\n",
        "\n",
        "    try:\n",
        "        model=load_model(fileandpath)\n",
        "        model.summary()\n",
        "\n",
        "        token_list, seq_length, step, total_words,start_story,tokenizer = load_data(\"localdata\")\n",
        "        print(1)\n",
        "        seedtext=\"till handelsman och\"\n",
        "        print(2)\n",
        "        newtext=generate_text2(seedtext,1,model,10,0.01,start_story,tokenizer)\n",
        "        print(3)\n",
        "        print(\"output from loaded model : (\",seedtext,\") => \", newtext)\n",
        "    except:\n",
        "        print(\"could not find \"+fileandpath)\n",
        "  \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn02w2GQ-jxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nextw(model,args,seedtext):\n",
        "    fileandpath = os.path.join(args.job_dir, 'rak_dnngit')+\"/allincluded.model\"\n",
        "\n",
        "    try:\n",
        "        token_list, seq_length, step, total_words,start_story,tokenizer = load_data(\"localdata\")\n",
        "        newtext=generate_text2(seedtext,1,model,10,0.01,start_story,tokenizer)\n",
        "        print(\"output from loaded model : (\",seedtext,\") => \", newtext)\n",
        "    except:\n",
        "        print(\"could not find \"+fileandpath)\n",
        "  \n",
        "    return newtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7LcTptZ8Wtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmpdirname='/content/tmp/rakdnngit'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIakwqKl8gO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xbatch_size=32\n",
        "xlearning_rate=0.01\n",
        "xnum_epochs=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X30AGaNS8QKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Argar:\n",
        "    pcsim = True\n",
        "    pcsimdir = tmpdirname\n",
        "    batch_size=xbatch_size\n",
        "    learning_rate=xlearning_rate\n",
        "    num_epochs=xnum_epochs\n",
        "    job_dir=tmpdirname+\"/jobdir\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gFozJvO8mG1",
        "colab_type": "code",
        "outputId": "fa44dc49-9ece-44d4-c839-9c2c496d3836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "argen=Argar()\n",
        "model=test_model(argen)\n",
        "print(\"*************** test main - done\")\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, None, 100)         8700      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 256)         365568    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 87)                22359     \n",
            "=================================================================\n",
            "Total params: 921,939\n",
            "Trainable params: 921,939\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "3\n",
            "words probablility [9.9996316e-01 3.5159901e-05 3.9764780e-07 3.5815566e-07 2.8522976e-07\n",
            " 1.9284377e-07 1.1318283e-07 6.3224803e-08 6.0528222e-08]\n",
            "words index= [23 44 76 58 64 39 57 69 77]\n",
            "4\n",
            "words in order= 23  :  av p= 0.99996316\n",
            "words in order= 44  :  ? p= 3.51599e-05\n",
            "words in order= 76  :  ville p= 3.976478e-07\n",
            "words in order= 58  :  ha p= 3.5815566e-07\n",
            "words in order= 64  :  ner p= 2.8522976e-07\n",
            "words in order= 39  :  ska p= 1.9284377e-07\n",
            "words in order= 57  :  något p= 1.1318283e-07\n",
            "words in order= 69  :  vara p= 6.32248e-08\n",
            "words in order= 77  :  fick p= 6.052822e-08\n",
            "words in order= 7  :  som p= 5.765485e-08\n",
            "words in order= 86  :  väl p= 5.6219896e-08\n",
            "words in order= 42  :  efter p= 4.974749e-08\n",
            "5\n",
            "3\n",
            "output from loaded model : ( till handelsman och ) =>  av\n",
            "*************** test main - done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0odN6zy-bQY",
        "colab_type": "code",
        "outputId": "39a9a16a-14f2-465d-8674-19a2686b6882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "nextw(model,argen,\"handelsman och ha\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "words probablility [9.7405690e-01 1.1880924e-02 8.4086424e-03 4.2987545e-03 7.5667095e-04\n",
            " 2.8790551e-04 1.3319797e-04 2.9075010e-05 1.8518569e-05]\n",
            "words index= [45 56  3 53 73 85 12 21 59]\n",
            "4\n",
            "words in order= 45  :  kunde p= 0.9740569\n",
            "words in order= 56  :  från p= 0.011880924\n",
            "words in order= 3  :  . p= 0.008408642\n",
            "words in order= 53  :  _ p= 0.0042987545\n",
            "words in order= 73  :  : p= 0.00075667095\n",
            "words in order= 85  :  andra p= 0.0002879055\n",
            "words in order= 12  :  med p= 0.00013319797\n",
            "words in order= 21  :  de p= 2.907501e-05\n",
            "words in order= 59  :  ja p= 1.8518569e-05\n",
            "words in order= 1  :  , p= 1.6006112e-05\n",
            "words in order= 7  :  som p= 9.351805e-06\n",
            "words in order= 70  :  här p= 8.5701495e-06\n",
            "5\n",
            "output from loaded model : ( handelsman och ha ) =>  kunde\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kunde'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}